import logging
from uuid import UUID
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File
from sqlalchemy import func, select
from shared.utils.redis_client import (RedisCacheService, cache_dashboard,
                                       cache_statistics, get_redis_cache, invalidate_dashboard_cache)
from shared.database.database import get_db, AsyncSession
from statistics_service.models.statistics import StatisticsDB
from statistics_service.schemas.dashboard import DashboardOverviewSchema
from statistics_service.schemas.statistics import (
    StatisticsRequest, StatisticsResponseSchema, FileStatisticsResponseSchema, ErrorResponse)
from statistics_service.services.dashboard_utils import get_bit_distribution_data, get_heatmap_data, get_worst_tests_data
from statistics_service.services.statistics_processor import StatisticsProcessor
from statistics_service.storage import get_minio_client, MinIOClient
from statistics_service.schemas.statistics import FileStatisticsRequest

router = APIRouter(prefix="/statistics", tags=["statistics"])

logger = logging.getLogger(__name__)

@router.post(
    "/sequence",
    response_model=StatisticsResponseSchema,
    status_code=status.HTTP_200_OK,
    responses={
        400: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def analyze_sequence(request: StatisticsRequest,
                           db: AsyncSession = Depends(get_db),
                           minio: MinIOClient = Depends(get_minio_client), 
                           redis_cache: RedisCacheService = Depends(get_redis_cache)):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Å—Ç—Ä–æ–∫–æ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    logger.info(f"Starting sequence analysis for sequence_id: {request.sequence_id}, "
                f"sequence_length: {len(request.sequence)}")
    
    try:
        logger.debug("Creating StatisticsProcessor instance")
        processor = StatisticsProcessor(db, minio)
        
        logger.info("Processing sequence statistics")
        result = await processor.process_sequence_statistics(request)
        
        logger.info(f"Successfully processed statistics. Statistics ID: {result.statistics_id}, "
                   f"Tests passed: {result.summary['tests_passed']}/{result.summary['tests_total']}")
        
        response_data = StatisticsResponseSchema(
            statistics_id=result.statistics_id,
            sequence_id=result.sequence_id,
            sequence_length=result.sequence_length,
            ones_count=result.ones_count,
            zeros_count=result.zeros_count,
            ones_proportion=result.ones_proportion,
            tests_passed=result.summary["tests_passed"],
            tests_total=result.summary["tests_total"],
            success_rate=result.summary["success_rate"],
            created_at=result.created_at,
            tests_results=result.tests_results
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        await redis_cache.set_cached_statistics(result.statistics_id, response_data.dict())
        
        await invalidate_dashboard_cache()
        
        logger.info(f"Successfully processed statistics. Statistics ID: {result.statistics_id}")
        
        return response_data
        
    except ValueError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Statistics processing error: {str(e)}"
        )

@router.post(
    "/file",
    response_model=FileStatisticsResponseSchema,
    status_code=status.HTTP_200_OK,
    responses={
        400: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def analyze_file(file: UploadFile = File(...),
                       db : AsyncSession = Depends(get_db),
                       minio : MinIOClient=Depends(get_minio_client),
                       redis_cache: RedisCacheService = Depends(get_redis_cache)):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ñ–∞–π–ª–∞"""
    try:
        processor = StatisticsProcessor(db,minio)
        content = await file.read()
        file_content = content.decode('utf-8')

        request = FileStatisticsRequest(file_content=file_content)
        result = await processor.process_file_statistics(request)
        
        response_data = FileStatisticsResponseSchema(
            statistics_id=result.statistics_id,
            sequence_id=result.sequence_id,
            file_url=result.summary.get("file_url"),
            tests_passed=result.summary["tests_passed"],
            tests_total=result.summary["tests_total"],
            success_rate=result.summary["success_rate"],
            created_at=result.created_at,
            tests_results=result.tests_results
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        await redis_cache.set_cached_statistics(result.statistics_id, response_data.dict())
        
        await invalidate_dashboard_cache()
        
        return response_data
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"File statistics processing error: {str(e)}"
        )

@cache_statistics(key_param="sequence_id", ttl=1800)
@router.get(
    "/{sequence_id}",
    response_model=StatisticsResponseSchema,
    status_code=status.HTTP_200_OK,
    responses={
        404: {"model": ErrorResponse},
        500: {"model": ErrorResponse}
    }
)
async def get_statistics(
    sequence_id: UUID,
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ ID"""
    print(f"Getting statistics by sequence_ID: {sequence_id}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –≤—ã–∑–æ–≤–æ–º (–¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞)
    logger.info(f"üì• –ó–ê–ü–†–û–° –î–ê–ù–ù–´–• –¥–ª—è sequence_id: {sequence_id}")
    
    try:
        logger.debug("Creating StatisticsProcessor instance")
        processor = StatisticsProcessor(db, None)  # MinIO –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è —á—Ç–µ–Ω–∏—è
        
        logger.info("Fetching statistics from database")
        result = await processor.get_statistics_by_id(sequence_id)
        
        if not result:
            logger.warning(f"Statistics not found for sequence_ID: {sequence_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Statistics record not found"
            )
        
        logger.info(f"‚úÖ –î–ê–ù–ù–´–ï –ü–û–õ–£–ß–ï–ù–´ –ò–ó –ë–î. Statistics ID: {result.statistics_id}")
        
        response_data = StatisticsResponseSchema(
            statistics_id=result.statistics_id,
            sequence_id=result.sequence_id,
            sequence_length=result.sequence_length,
            ones_count=result.ones_count,
            zeros_count=result.zeros_count,
            ones_proportion=result.ones_proportion,
            tests_passed=result.summary["tests_passed"],
            tests_total=result.summary["tests_total"],
            success_rate=result.summary["success_rate"],
            created_at=result.created_at,
            tests_results=result.tests_results
        )
        
        logger.info(f"üéØ –û–¢–í–ï–¢ –°–§–û–†–ú–ò–†–û–í–ê–ù –¥–ª—è {sequence_id}")
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ statistics –¥–ª—è {sequence_id}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fetching statistics: {str(e)}"
        )
@cache_dashboard(ttl=300)
@router.get(
    "/dashboard/overview",
    response_model=DashboardOverviewSchema,
    status_code=status.HTTP_200_OK
)
async def get_dashboard_overview(db: AsyncSession = Depends(get_db)):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
    logger.info("Fetching dashboard overview data")
    
    try:
        # 1. –í—Å–µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        total_sequences_query = select(func.count(StatisticsDB.id))
        total_result = await db.execute(total_sequences_query)
        total_sequences = total_result.scalar()

        # 2. –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        avg_length_query = select(func.avg(StatisticsDB.sequence_length))
        avg_length_result = await db.execute(avg_length_query)
        avg_sequence_length = round(avg_length_result.scalar() or 0, 2)

        # 3. –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π —É—Å–ø–µ—Ö –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
        avg_success_query = select(func.avg(StatisticsDB.summary['success_rate'].as_float()))
        avg_success_result = await db.execute(avg_success_query)
        avg_success_rate = round(avg_success_result.scalar() or 0, 2)

        # 4. –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è 0 –∏ 1
        bit_distribution_data = await get_bit_distribution_data(db)

        # 5. –î–≤–∞ —Å–∞–º—ã—Ö –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–∞
        worst_tests_data = await get_worst_tests_data(db)

        # 6. –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ö–∏—Ç–º–∞–ø–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        heatmap_data = await get_heatmap_data(db)

        return DashboardOverviewSchema(
            total_sequences=total_sequences,
            avg_sequence_length=avg_sequence_length,
            avg_success_rate=avg_success_rate,
            bit_distribution=bit_distribution_data,
            worst_tests=worst_tests_data,
            heatmap_data=heatmap_data
        )
        
    except Exception as e:
        logger.error(f"Error fetching dashboard data: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Dashboard data fetching error: {str(e)}"
        )
